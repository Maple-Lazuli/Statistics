---
title: "STAT500"
author: "Ada Lazuli"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: spacelab
    highlight: tango
    toc: true
    toc_depth: 3
    toc_float: true
    code_folding: hide
    code-tools: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("/home/maple/CodeProjects/Statistics/STAT500")
```

```{r collapse=TRUE}
library(dplyr)
library(ggplot2)
library(qqplotr)
```

# Collecting and Summarizing Data

## Numerical Summarization

**Scenario:** The College of Dentistry at the University of Florida has made a commitment to develop its entire curriculum around the use of self-paced instructional mate-rials such as video tapes, slide tapes, and syllabi. It is hoped that each student will proceed at a pace commensurate with his or her ability and that the instructional staﬀ will have more free time for personal consultation in student-faculty interactions. One such instructional module was developed and tested on the ﬁrst 50 students proceeding through the curriculum. The following measurements represent the number of hours it took these students to complete the required modular material.

**Data:** 
```
16 8 33 21 34 17 12 14 27 6 33 25 16 7 15 18 25 29
19 27 5 12 29 22 14 25 21 17 9 4 12 15 13 11 6 9 26
5 16 5 9 11 5 4 5 23 21 10 17 15
```

```{r}
data <- c(16,8,33,21,34,17,12,14,27,6,33,25,16,7,15,18,25,29,19,27,5,
          12,29,22,14,25,21,17,9,4,12,15,13,11,6,9,26,5,16,5,9,11,5,4,
          5,23,21,10,17,15)
```

To get the 5-Number summary plus the mean, use the `summary` function.

```{r}
summary(data)
```


The *empirical method* to get the standard deviation:

```{r}
empirical_std <- (max(data) - min(data))/4
empirical_std
```

To use R functions for standard deviation:

```{r}
sd(data)
```

To get the **coefficient of variation (CV)**:

```{r}
cv <- sd(data)/mean(data)
cv
```

## Visualization

How to make a pie chart

```{r}
# example two feature data.
df_pie <- data.frame(
  category = c("A", "B", "C", "D"),
  value = c(10, 20, 30, 40)
)

ggplot(df_pie, aes(x = "", y = value, fill = category)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar(theta = "y") +
  theme_void() +
  labs(title = "Pie Chart Example")
```


How to make a histogram
```{r collapse=TRUE}
df = data.frame(values = data)
ggplot(df, aes(x = values)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Histogram Example", x = "Value", y = "Count")
```

How to make a box plot
```{r collapse=TRUE}
ggplot(df, aes(x = values)) +
  geom_boxplot() +
  labs(title = "Boxplot Example")
```

How to make a probability plot
```{r collapse=TRUE}
ggplot(df, aes(sample = values)) +
  stat_qq_band(distribution = "norm", alpha = 0.2, fill = "lightblue") +
  #stat_qq_line(distribution = "norm", color = "red") +
  stat_qq_point(distribution = "norm", color = "blue") +
  labs(title = "Normal Probability Plot (with Confidence Band)",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles") +
  theme_minimal()
```

# Probability Distributions

## Binomial Distribution

**Scenario:** In an attempt to decrease drunk driving, police set up vehicle checkpoints during the July 4 evening. The police randomly select vehicles to be stopped for informational checks. On a particular roadway, assume that 20% of all drivers have a blood alcohol level above the legal limit. For a random sample of 15 vehicles, compute the following probabilities:

(a) All 15 drivers will have a blood alcohol level exceeding the legal limit.

```{r}
.2**15
```

The probability of all 15 drivers having a blood alcohol level above the limit is `r 0.2**15`

(b) Exactly 6 of the 15 drivers will exceed the legal limit.

```{r}
dbinom(x = 6, size = 15, prob = .2)
```
(c) Of the 15 drivers, more than 1 will exceed the legal limit.

```{r}
one_or_fewer <- pbinom(1, size = 15, prob = .2)
1 - one_or_fewer
```

(d) All 15 drivers will have a blood alcohol level within the legal limit.
```{r}
pbinom(0, size = 15, prob = .2)
```

## Normal Distribution

**Scenario:** Monthly sales ﬁgures for a particular food industry tend to be normally distributed with a mean of 150 (thousand dollars) and a standard deviation of 35 (thousand dollars). Compute the following probabilities:

(a) $P(X < 200)$

```{r collapse=TRUE}
mean <- 150
x <- 200
sd <- 35
z <- (x - mean)/sd
pnorm(z)
```

(b) $P(X > 100)$

```{r collapse=TRUE}
mean <- 150
x <- 100
sd <- 35
z <- (x - mean)/sd
1 - pnorm(z)
```

(b) $P(100 < X < 200)$

```{r collapse=TRUE}
mean <- 150
x1 <- 100
x2 <- 200
sd <- 35
z1 <- (x1 - mean)/sd
z2 <- (x2 - mean)/sd

pnorm(z2) - pnorm(z1)
```

# Sampling Distributions

## Sample Proportion Example

**Scenario:** The company JCrew advertises that 95% of its online orders ship within two working days. You select a random sample of 200 of the 10,000 orders received over the past month to audit. The audit reveals that 180 of these orders shipped on time.

(a) What is the sample proportion of orders shipped on time?

```{r}
p_hat <- 180/200
```

$\hat{p} = \frac{180}{200} =$ `r p_hat`

(b) Does the sample data satisfy conditions necessary for the sample proportion to follow an approximately normal distribution?

Checking the conditions:

1. $n \cdot p > 15$
2. $n \cdot (1 - p) > 15$

```{r}
200 * p_hat > 15
```

```{r}
200 * (1 - p_hat) > 15
```


(c) What is the mean and standard error (SE) of the sample distribution assuming normal?

```{r collapse=TRUE}
p_null <- 0.95
se <- sqrt(p_null * (1 - p_null)/200)
```

$SE = \sqrt{\frac{\hat{p} \cdot (1 - \hat{p})}{n}} =$ `r se``

(d) If JCrew really ships 95% of its orders on time, what is probability that the proportion in a random sample of 200 orders is as small or smaller as the proportion in the audit?

```{r collapse=TRUE}
z <- (p_hat - p_null)/se
pnorm(z)
```

$P(\hat{p} \leq 0.9) = P(Z \leq \frac{\hat{p} - p_0}{SE}) = P(Z \leq$ `r z`$) =$ `r pnorm(z)`

(e) If we treated the problem as a binomial, how would the problem be set up? That is, what would we want to find the probability of?

It would be a binomial with `x = 180`, `size = 200`, and `prob = 0.95`

### Sample Mean Example

**Scenario:** Penn State Fleet which operates and manages car rentals for Penn State employees found that the tire lifetime for their vehicles has a mean of 50,000 miles and standard deviation of 3500 miles.

(a) What would be the distribution, mean and standard error mean lifetime of a random sample of 50 vehicles?

Given that there are in excess of 30 samples, the sampling distribution can be assumed to be normal.

It would be expected to have a mean of 50000 and standard error of $SE = \frac{\sigma}{\sqrt{n}} = $ `r 3500/sqrt(50)`

(b) What is the probability that the sample mean lifetime for these 50 vehicles exceeds 52,000?

```{r collapse=TRUE}
x <- 52000
mean <- 50000
se <- 3500/sqrt(50)
z <- (x - mean)/se
1 - pnorm(z)
```

$P(\bar{x} > 52000) = 1 - P(\bar{x} \leq 52000) = 1 -  P(Z < \frac{\bar{x} - \mu}{SE}) = 1 - P(Z <$ `r z`$) =$ `r 1 - pnorm(z)`


# Confidence Intervals

## Proportions

**Scenario:** A random sample of 1,200 units is randomly selected from a population. If there are 732 successes in the 1,200 draws,

(a) Construct a 95% conﬁdence interval for $p$.

```{r collapse=TRUE}
p_hat = 732/1200
alpha <- (100 - 95)/100
alpha_2 <- alpha/2
z_a2 <- qnorm(1 - alpha_2)
SE <- sqrt(p_hat * (1 - p_hat)/1200)
lower_limit <- p_hat - z_a2 * SE
upper_limit <- p_hat + z_a2 * SE
```

$\hat{p} \pm z_{a/2}\cdot \sqrt{\frac{\hat{p}\cdot (1 - \hat{p})}{n}} =$ `r p_hat` $\pm$ `r z_a2` $\cdot$ `r SE` $\rightarrow$ (`r lower_limit`, `r upper_limit`)

(b) Construct a 99% conﬁdence interval for $p$.

```{r collapse=TRUE}
p_hat = 732/1200
alpha <- (100 - 99)/100
alpha_2 <- alpha/2
z_a2 <- qnorm(1 - alpha_2)
SE <- sqrt(p_hat * (1 - p_hat)/1200)
lower_limit <- p_hat - z_a2 * SE
upper_limit <- p_hat + z_a2 * SE
```

$\hat{p} \pm z_{a/2}\cdot \sqrt{\frac{\hat{p}\cdot (1 - \hat{p})}{n}} =$ `r p_hat` $\pm$ `r z_a2` $\cdot$ `r SE` $\rightarrow$ (`r lower_limit`, `r upper_limit`)

(c) Explain the diﬀerence in the interpretation of these two conﬁdence intervals.

The intervals change due to the change in $z_{a/2}$. The interpretation difference is an increase in the confidence of where the true population proportion is.

## Means

**Scenario:**

Consumer reports tested 15 brands of vanilla yogurt and found the following numbers of calories per serving: 160, 200, 220, 230, 120, 180, 140, 130, 170, 180, 80, 120, 100, 170, 190. The sample statistics were 159.3 for the sample mean and 43.5 for the standard deviation.

(a) By hand, place a 99% confidence interval on the average number of calories per serving for vanilla yogurt.

```{r collapse=TRUE}
data <- c(160, 200, 220, 230, 120, 180, 140, 130, 170, 180, 80, 120, 100, 170, 190)
x_bar <- mean(data)
SE <- sd(data)/sqrt(length(data))


alpha <- (100 - 99)/100
alpha_2 <- alpha/2
t_a2 <- qt(1 - alpha_2, df = length(data) - 1)
lower_limit <- x_bar - t_a2 * SE
upper_limit <- x_bar + t_a2 * SE
```

$\bar{x} \pm t_{a/2}\cdot \frac{s}{\sqrt{n}}$ `r x_bar` $\pm$ `r t_a2` $\cdot$ `r SE` $\rightarrow$ (`r lower_limit`, `r upper_limit`)


(b) Provide an interpretation of your interval.

The researchers are 99% confident that the mean calories in vanilla yogurt is from 125.87
to 192.73

(c) Use R check the assumption of normality. Is the assumption satisfied? Explain.

```{r collapse=TRUE}
ggplot(data.frame(values = data), aes(sample = values)) +
  stat_qq_band(distribution = "norm", alpha = 0.2, fill = "lightblue") +
  stat_qq_line(distribution = "norm", color = "red", alpha = .2) +
  stat_qq_point(distribution = "norm", color = "blue") +
  labs(title = "Normal Probability Plot (with Confidence Band)",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles") +
  theme_minimal()
```

Since all of the values are within the confidence band, the assumption of normality is met.

# Hypothesis Testing with One Sample

## 1 Sample Proportion

**Scenario:** Chronic pain is often deﬁned as pain that occurs constantly and ﬂares up frequently, is not caused by cancer, and is experienced at least once a month for a one-year period of time. Many articles have been written about the relation-ship between chronic pain and the age of the patient. In a survey conducted on behalf of the American Chronic Pain Association in 2004, a random cross section of 800 adults who suﬀer from chronic pain found that 424 of the 800 participants in the survey were above the age of 50. Using the data in the survey, is there substantial evidence ($\alpha = 0.05$
) that more than half of persons suﬀering from chronic pain are over 50 years of age?

Hypotheses:

$H_0: p \leq 0.5$

$H_a: p > 0.5$

Assumption Verification:

$n \cdot p_0 > 15$
```{r}
n <- 800
p_null <- 0.5
n * p_null > 15
```

$n \cdot (1 - p_0) > 15$
```{r}
n * (1 - p_null) > 15
```

Test Statistic:

```{r}
alpha <- 0.05
p_hat <- 424/n
se <- sqrt(p_null * (1 - p_null)/n)
z <- p_hat - p_null/se
z
```

Getting the rejection region:

```{r}
qnorm(1 - alpha)
```

Getting the p-value:

$p\_value = P(Z >$ `r z`$) = 1 - P(Z \leq$ `r z`$) =$ `r 1 - pnorm(z)`

Conclusion:

Is $z^* > z_{a}$?

`r z` > `r qnorm(1 - alpha)`

```{r}
z > qnorm(1 - alpha)
```

Is the p-value less than $\alpha$?

`r 1 - pnorm(z)` < `r alpha`

```{r}
1 - pnorm(z) < alpha 
```
So, at 5% signiﬁcance level, there is suﬃcient evidence to conclude that more than half of persons suﬀering from chronic pain are over 50 years of age.

## 1 Sample Mean

A study was conducted of 90 adult male patients following a new treatment for congestive heart failure. One of the variables measured on the patients was the increase in exercise capacity (in minutes) over a 4-week treatment period. The previous treatment regime had produced an average increase of $\mu$= 2 minutes. 

The researchers wanted to evaluate whether the new treatment had increased the value of $\mu$ in comparison to the previous treatment. The data yielded $\bar{x} = 2.17$ and $s = 1.05$. Using $\alpha=0.05$, what conclusions can you draw about the research hypothesis using rejection region approach?

Hypotheses:

$H_0: \mu \leq 2$

$H_a: \mu > 2$

Assumption Verification:

Since $n > 30$, we can assume this sample follows a normal distribution.

Test Statistic:

```{r}
alpha <- 0.05
n <- 90
x_bar <- 2.17
s <- 1.05
se <- s / sqrt(n)

t <- (x_bar - 2)/se
t
```

Getting the rejection region:

```{r}
qt(1 - alpha, df = n - 1) #1 - alpha since this is a right tailed test.
```

Getting the p-value:

$p\_value = P(T_{89, 0.05} >$ `r t`$) = 1 - P(T_{89, 0.05} \leq$ `r t`$) =$ `r 1 - pt(t, df = n - 1)`

Conclusion:

Is $t^* > T_{89, 0.05}$?

`r t` > `r qt(1 - alpha, df = n - 1)`

```{r}
t > qt(1 - alpha, df = n - 1)
```

Is the p-value less than $\alpha$?

`r 1 - pt(t, df = n - 1)` < `r alpha`

```{r}
1 - pt(t, df = n - 1) < alpha 
```
 
 At a significance level of 5%, we can conclude that the data does not support the hypothesis that the mean has been increased from 2.

# Hypothesis Testing with Two Samples

## Two Proportions

**Scenario:** Sludge is a dried product remaining from processed sewage and is often used as a fertilizer on agriculture crops. If the sludge contains a high concentration of certain heavy metals, such as nickel, the nickel may be at a concentration in the crops to be of danger to the consumer of the crop. A new method of processing sewage has been developed and an experiment is conducted to evaluate its effectiveness in removing heavy metals. Sewage of a known concentration of nickel is treated using both the new and old methods. 

One hundred tomato plants were randomly assigned to pots containing sewage sludge processed by one of the two methods. The tomatoes harvested from the plants were evaluated to determine if the nickel was at a toxic level. The results are as follows:

| Treatment&nbsp;&nbsp;       | Toxic&nbsp;&nbsp;  | Non-toxic&nbsp;&nbsp; | Total |
|:-----------------:|:---------:|:-------------:|:---------:|
| New             | 5     | 45        | 50    |
| Old             | 9     | 41        | 50    |
| *Total*         | 14    | 86        | 100   |

Hypotheses:

$H_0: p_1 = p_2$

$H_a: p_1 - p_2 < 0$

Assumption Verification:

```{r}
p_1 <- 5/50
p_2 <- 9/50
p_hat <- 14/100
assumptions <- c(50*p_1 >= 5, 50*(1- p_1) >= 5, 50*p_2 >= 5, 50*(1- p_2) >= 5)
assumptions
```


Test Statistic:

$z^* = \frac{p_1 - p_2}{\sqrt{\hat{p} (1 - \hat{p}(\frac{1}{n_1} + \frac{1}{n_2}))}}$

```{r}
alpha <- 0.05

SE <- sqrt(p_hat * (1 - p_hat) * (1/50 + 1/50))

z = (p_1 - p_2)/SE
z
```

Getting the rejection region:

```{r}
qnorm(alpha) # do not subtract 1 from alpha here since this is a left tailed test.
```

Getting the p-value:

$p\_value = P(Z \leq$ `r z`$) =$ `r pnorm(z)`

Conclusion:

Is $z^* < z_{a}$? 

(*note* the less than sign here due to this being a left tail test.)

`r z` < `r qnorm(alpha)`

```{r}
z < qnorm(alpha)
```

Is the p-value less than $\alpha$?

`r pnorm(z)` < `r alpha`

```{r}
pnorm(z) < alpha 
```
 
At a significance level of 5%, it seems that there is not significant evidence that the new treatment would have a lower proportion of plants having toxic levels of nickle.

## Two Means

According to the publication, High School Profile Report, in past years college-bound males have out-performed college-bound females on the mathematics portion of tests given by ACT Program. Samples of this year’s score yield the following data.

| Group   | C1 | C2 | C3 | C4 | C5 | C6 | C7 | C8 | C9 | C10 | C11 | C12 | C13 | C14 |
|:-------:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:---:|:---:|:---:|:---:|:---:|
| Males   | 34 | 34 | 30 | 18 | 24 | 16 | 15 | 26 | 13 | 21  | 24  | 11  | 11  | 15  |
| Females | 18 | 33 | 27 | 11 | 23 | 23 | 18 | 20 | 26 | 10  | 20  | 22  | 14  | 21  |


Does it appear that college-bound males are, on the average, still outperforming college-bound females on the mathematics portion of ACT tests? Set up the hypothesis. Indicate whether you are going to use 2-sample t-test or paired t-test for the problem. If you use 2-sample t-test, will you use pooled or non-pooled variances? What conditions will you check before you perform the test?

Hypotheses:

$H_0: \mu_{male} - \mu_{female} \leq 0$

$H_a: \mu_{male} - \mu_{female} > 0$

Assumptions:

Start by storing the study observations into a dataframe.

```{r}
males <- c(34,34,30,18,24,16,15,26,13,21,24,11,11,15)
females <- c(18,33,27,11,23,23,18,20,26,10,20,22,14,21)
df <- rbind(data.frame(values = males, group = "male"),
            data.frame(values = females, group = "female"))
```

The first assumption to check is whether we can assume equal variances. We
assess this with the rule of thumb: $0.5 < \frac{s_1}{s_2} <2.0$

In this instance: $\frac{s_1}{s_2} =$ `r sd(males)/sd(females)`

Since the ratio is within our threshold, we can proceed with pool variance.

The second assumption to check is whether the two sets follow a normal 
distribution.

```{r}
ggplot(df, aes(sample = values, color = group, fill = group)) +
  stat_qq_band(distribution = "norm", alpha = 0.1) +
  stat_qq_line(distribution = "norm", color = "red", alpha = .2) +
  stat_qq_point(distribution = "norm", color = "blue") +
  labs(title = "Normal Probability Plot (with Confidence Band)",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles") +
  facet_wrap(~group)+
  theme_minimal()
```

Test Statistic:

$t^* = \frac{\bar{x} - \bar{x}}{s_p \cdot \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}$

where,

$s_p = \sqrt frac{(n_1 -1)\cdot s_1^2 + (n_2 - 1 \cdot s^2_2)}{n_1 + n_2 -2}$

and 

$df = n_1 + n_2 -2$

```{r}
alpha <- 0.05
df <- length(males) + length(females) - 2
top = (length(males) - 1)*sd(males)**2 + (length(females) - 1)*sd(females)**2
s_p = sqrt(top/df)

t = (mean(males) - mean(females))/(s_p*sqrt(1/length(males)+1/length(females)))
t
```

Getting the rejection region:

```{r}
qt(1 - alpha, df)
```

Getting the p-value:

$p\_value = P(T_{26, 0.05} >$ `r t`$) =$ `r 1 - qt(t, df)`

Conclusion:

Is $t^* > T_{26, 0.05}$? 

`r t` > `r qt(1 - alpha, df)`

```{r}
t > qt(1 - alpha, df)
```

Is the p-value less than $\alpha$?

`r 1 - pt(t, df)` < `r alpha`

```{r}
1 - pt(t, df) < alpha 
```
 
At a significance level of 5%, there is not enough evidence to conclude that, on average, college-bound males score higher on the ACT Math compared to college-bound females.

## Paired Means

**Scenario:** Eleven tires were each measured for tread wear by two methods, one based on weight and the other on groove wear. Here is the data in thousands of miles:

| Group  | C1   | C2   | C3   | C4   | C5   | C6   | C7   | C8   | C9   | C10  | C11  |
|:------:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|
| Weight | 30.5 | 30.9 | 31.9 | 30.4 | 27.3 | 20.4 | 24.5 | 20.9 | 18.9 | 13.7 | 11.4 |
| Groove | 28.7 | 25.9 | 23.3 | 23.1 | 23.7 | 20.9 | 16.1 | 19.9 | 15.2 | 11.5 | 11.2 |

Does it appear the two methods, on the average, give different results? Set up the hypotheses. Indicate whether you are going to use 2-sample t-test or paired t-test for the problem. If you use 2-sample t-test, will you use pooled or non-pooled variances? What conditions will you check before you perform the test?

Hypotheses:

$H_0: \mu_d = 0$

$H_a: \mu_d \neq 0$

Assumptions:

Verify that $d$ follows a normal distribution

```{r}
Weight <- c(30.5,30.9,31.9,30.4,27.3,20.4,24.5,20.9,18.9,13.7,11.4)
Groove <- c(28.7,25.9,23.3,23.1,23.7,20.9,16.1,19.9,15.2,11.5,11.2)
d = Weight - Groove

ggplot(data.frame(values = d), aes(sample = values)) +
  stat_qq_band(distribution = "norm", alpha = 0.2, fill = "lightblue") +
  stat_qq_line(distribution = "norm", color = "red", alpha = .2) +
  stat_qq_point(distribution = "norm", color = "blue") +
  labs(title = "Normal Probability Plot (with Confidence Band)",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles") +
  theme_minimal()
```

Test Statistic:

The test statistic is given by

$t^* =\frac{\bar{d}}{\frac{s_d}{\sqrt{n}}}$

with the degrees of freedom being given by $df = n(pairs)  - 1$

```{r}
alpha = 0.05/2 # since this is a two tailed test.
t = mean(d)/(sd(d)/sqrt(length(d)))

df = length(d) - 1

t
```

Getting the rejection region:

```{r}
qt(1 - alpha, df)
```

Getting the p-value:

$p\_value = P(T_{10, 0.025} >|$ `r t`$|) =$ `r 1 - qt(t, df)`

Conclusion:

Is $t^* > T_{10, 0.05}$? 


`r t` > `r qt(1 - alpha, df)`

```{r}
t > qt(1 - alpha, df)
```

Is the p-value less than $\alpha$?

`r 1 - pt(t, df)` < `r alpha`

```{r}
1 - pt(t, df) < alpha 
```
 
There is statistical evidence to conclude that there is a difference in average tread wear between the weight and groove methods.


# Chi-Square Test for Independence

# Linear Regression

# Analysis of Variance
