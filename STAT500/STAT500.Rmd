---
title: "STAT500"
author: "Ada Lazuli"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: spacelab
    highlight: tango
    toc: true
    toc_depth: 3
    toc_float: true
#    code_folding: hide
    code-tools: true
  pdf_document:
    toc: true
    toc_depth: '3'
  word_document:
    toc: true
    toc_depth: '3'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("/home/maple/CodeProjects/Statistics/STAT500")
```

```{r collapse=TRUE}
library(dplyr)
library(ggplot2)
library(qqplotr)
library(tidyr)
library(reactable)
library(openxlsx)
```

# Collecting and Summarizing Data

## Numerical Summarization

**Scenario:** The College of Dentistry at the University of Florida has made a commitment to develop its entire curriculum around the use of self-paced instructional mate-rials such as video tapes, slide tapes, and syllabi. It is hoped that each student will proceed at a pace commensurate with his or her ability and that the instructional staﬀ will have more free time for personal consultation in student-faculty interactions. One such instructional module was developed and tested on the ﬁrst 50 students proceeding through the curriculum. The following measurements represent the number of hours it took these students to complete the required modular material.

**Data:** 
```
16 8 33 21 34 17 12 14 27 6 33 25 16 7 15 18 25 29
19 27 5 12 29 22 14 25 21 17 9 4 12 15 13 11 6 9 26
5 16 5 9 11 5 4 5 23 21 10 17 15
```

```{r}
data <- c(16,8,33,21,34,17,12,14,27,6,33,25,16,7,15,18,25,29,19,27,5,
          12,29,22,14,25,21,17,9,4,12,15,13,11,6,9,26,5,16,5,9,11,5,4,
          5,23,21,10,17,15)
```

To get the 5-Number summary plus the mean, use the `summary` function.

```{r}
summary(data)
```


The *empirical method* to get the standard deviation:

```{r}
empirical_std <- (max(data) - min(data))/4
empirical_std
```

To use R functions for standard deviation:

```{r}
sd(data)
```

To get the **coefficient of variation (CV)**:

```{r}
cv <- sd(data)/mean(data)
cv
```

## Visualization

How to make a pie chart

```{r}
# example two feature data.
df_pie <- data.frame(
  category = c("A", "B", "C", "D"),
  value = c(10, 20, 30, 40)
)

ggplot(df_pie, aes(x = "", y = value, fill = category)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar(theta = "y") +
  theme_void() +
  labs(title = "Pie Chart Example")
```


How to make a histogram
```{r collapse=TRUE}
df = data.frame(values = data)
ggplot(df, aes(x = values)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Histogram Example", x = "Value", y = "Count")
```

How to make a box plot
```{r collapse=TRUE}
ggplot(df, aes(x = values)) +
  geom_boxplot() +
  labs(title = "Boxplot Example")
```

How to make a probability plot
```{r collapse=TRUE}
ggplot(df, aes(sample = values)) +
  stat_qq_band(distribution = "norm", alpha = 0.2, fill = "lightblue") +
  #stat_qq_line(distribution = "norm", color = "red") +
  stat_qq_point(distribution = "norm", color = "blue") +
  labs(title = "Normal Probability Plot (with Confidence Band)",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles") +
  theme_minimal()
```

# Probability Distributions

## Binomial Distribution

**Scenario:** In an attempt to decrease drunk driving, police set up vehicle checkpoints during the July 4 evening. The police randomly select vehicles to be stopped for informational checks. On a particular roadway, assume that 20% of all drivers have a blood alcohol level above the legal limit. For a random sample of 15 vehicles, compute the following probabilities:

(a) All 15 drivers will have a blood alcohol level exceeding the legal limit.

```{r}
.2**15
```

The probability of all 15 drivers having a blood alcohol level above the limit is `r 0.2**15`

(b) Exactly 6 of the 15 drivers will exceed the legal limit.

```{r}
dbinom(x = 6, size = 15, prob = .2)
```
(c) Of the 15 drivers, more than 1 will exceed the legal limit.

```{r}
one_or_fewer <- pbinom(1, size = 15, prob = .2)
1 - one_or_fewer
```

(d) All 15 drivers will have a blood alcohol level within the legal limit.
```{r}
pbinom(0, size = 15, prob = .2)
```

## Normal Distribution

**Scenario:** Monthly sales ﬁgures for a particular food industry tend to be normally distributed with a mean of 150 (thousand dollars) and a standard deviation of 35 (thousand dollars). Compute the following probabilities:

(a) $P(X < 200)$

```{r collapse=TRUE}
mean <- 150
x <- 200
sd <- 35
z <- (x - mean)/sd
pnorm(z)
```

(b) $P(X > 100)$

```{r collapse=TRUE}
mean <- 150
x <- 100
sd <- 35
z <- (x - mean)/sd
1 - pnorm(z)
```

(b) $P(100 < X < 200)$

```{r collapse=TRUE}
mean <- 150
x1 <- 100
x2 <- 200
sd <- 35
z1 <- (x1 - mean)/sd
z2 <- (x2 - mean)/sd

pnorm(z2) - pnorm(z1)
```

# Sampling Distributions

## Sample Proportion Example

**Scenario:** The company JCrew advertises that 95% of its online orders ship within two working days. You select a random sample of 200 of the 10,000 orders received over the past month to audit. The audit reveals that 180 of these orders shipped on time.

(a) What is the sample proportion of orders shipped on time?

```{r}
p_hat <- 180/200
```

$\hat{p} = \frac{180}{200} =$ `r p_hat`

(b) Does the sample data satisfy conditions necessary for the sample proportion to follow an approximately normal distribution?

Checking the conditions:

1. $n \cdot p > 15$
2. $n \cdot (1 - p) > 15$

```{r}
200 * p_hat > 15
```

```{r}
200 * (1 - p_hat) > 15
```


(c) What is the mean and standard error (SE) of the sample distribution assuming normal?

```{r collapse=TRUE}
p_null <- 0.95
se <- sqrt(p_null * (1 - p_null)/200)
```

$SE = \sqrt{\frac{\hat{p} \cdot (1 - \hat{p})}{n}} =$ `r se``

(d) If JCrew really ships 95% of its orders on time, what is probability that the proportion in a random sample of 200 orders is as small or smaller as the proportion in the audit?

```{r collapse=TRUE}
z <- (p_hat - p_null)/se
pnorm(z)
```

$P(\hat{p} \leq 0.9) = P(Z \leq \frac{\hat{p} - p_0}{SE}) = P(Z \leq$ `r z`$) =$ `r pnorm(z)`

(e) If we treated the problem as a binomial, how would the problem be set up? That is, what would we want to find the probability of?

It would be a binomial with `x = 180`, `size = 200`, and `prob = 0.95`

### Sample Mean Example

**Scenario:** Penn State Fleet which operates and manages car rentals for Penn State employees found that the tire lifetime for their vehicles has a mean of 50,000 miles and standard deviation of 3500 miles.

(a) What would be the distribution, mean and standard error mean lifetime of a random sample of 50 vehicles?

Given that there are in excess of 30 samples, the sampling distribution can be assumed to be normal.

It would be expected to have a mean of 50000 and standard error of $SE = \frac{\sigma}{\sqrt{n}} = $ `r 3500/sqrt(50)`

(b) What is the probability that the sample mean lifetime for these 50 vehicles exceeds 52,000?

```{r collapse=TRUE}
x <- 52000
mean <- 50000
se <- 3500/sqrt(50)
z <- (x - mean)/se
1 - pnorm(z)
```

$P(\bar{x} > 52000) = 1 - P(\bar{x} \leq 52000) = 1 -  P(Z < \frac{\bar{x} - \mu}{SE}) = 1 - P(Z <$ `r z`$) =$ `r 1 - pnorm(z)`


# Confidence Intervals

## Proportions

**Scenario:** A random sample of 1,200 units is randomly selected from a population. If there are 732 successes in the 1,200 draws,

(a) Construct a 95% conﬁdence interval for $p$.

```{r collapse=TRUE}
p_hat = 732/1200
alpha <- (100 - 95)/100
alpha_2 <- alpha/2
z_a2 <- qnorm(1 - alpha_2)
SE <- sqrt(p_hat * (1 - p_hat)/1200)
lower_limit <- p_hat - z_a2 * SE
upper_limit <- p_hat + z_a2 * SE
```

$\hat{p} \pm z_{a/2}\cdot \sqrt{\frac{\hat{p}\cdot (1 - \hat{p})}{n}} =$ `r p_hat` $\pm$ `r z_a2` $\cdot$ `r SE` $\rightarrow$ (`r lower_limit`, `r upper_limit`)

(b) Construct a 99% conﬁdence interval for $p$.

```{r collapse=TRUE}
p_hat = 732/1200
alpha <- (100 - 99)/100
alpha_2 <- alpha/2
z_a2 <- qnorm(1 - alpha_2)
SE <- sqrt(p_hat * (1 - p_hat)/1200)
lower_limit <- p_hat - z_a2 * SE
upper_limit <- p_hat + z_a2 * SE
```

$\hat{p} \pm z_{a/2}\cdot \sqrt{\frac{\hat{p}\cdot (1 - \hat{p})}{n}} =$ `r p_hat` $\pm$ `r z_a2` $\cdot$ `r SE` $\rightarrow$ (`r lower_limit`, `r upper_limit`)

(c) Explain the diﬀerence in the interpretation of these two conﬁdence intervals.

The intervals change due to the change in $z_{a/2}$. The interpretation difference is an increase in the confidence of where the true population proportion is.

## Means

**Scenario:**

Consumer reports tested 15 brands of vanilla yogurt and found the following numbers of calories per serving: 160, 200, 220, 230, 120, 180, 140, 130, 170, 180, 80, 120, 100, 170, 190. The sample statistics were 159.3 for the sample mean and 43.5 for the standard deviation.

(a) By hand, place a 99% confidence interval on the average number of calories per serving for vanilla yogurt.

```{r collapse=TRUE}
data <- c(160, 200, 220, 230, 120, 180, 140, 130, 170, 180, 80, 120, 100, 170, 190)
x_bar <- mean(data)
SE <- sd(data)/sqrt(length(data))


alpha <- (100 - 99)/100
alpha_2 <- alpha/2
t_a2 <- qt(1 - alpha_2, df = length(data) - 1)
lower_limit <- x_bar - t_a2 * SE
upper_limit <- x_bar + t_a2 * SE
```

$\bar{x} \pm t_{a/2}\cdot \frac{s}{\sqrt{n}}$ `r x_bar` $\pm$ `r t_a2` $\cdot$ `r SE` $\rightarrow$ (`r lower_limit`, `r upper_limit`)


(b) Provide an interpretation of your interval.

The researchers are 99% confident that the mean calories in vanilla yogurt is from 125.87
to 192.73

(c) Use R check the assumption of normality. Is the assumption satisfied? Explain.

```{r collapse=TRUE}
ggplot(data.frame(values = data), aes(sample = values)) +
  stat_qq_band(distribution = "norm", alpha = 0.2, fill = "lightblue") +
  stat_qq_line(distribution = "norm", color = "red", alpha = .2) +
  stat_qq_point(distribution = "norm", color = "blue") +
  labs(title = "Normal Probability Plot (with Confidence Band)",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles") +
  theme_minimal()
```

Since all of the values are within the confidence band, the assumption of normality is met.

# Hypothesis Testing with One Sample

## 1 Sample Proportion

**Scenario:** Chronic pain is often deﬁned as pain that occurs constantly and ﬂares up frequently, is not caused by cancer, and is experienced at least once a month for a one-year period of time. Many articles have been written about the relation-ship between chronic pain and the age of the patient. In a survey conducted on behalf of the American Chronic Pain Association in 2004, a random cross section of 800 adults who suﬀer from chronic pain found that 424 of the 800 participants in the survey were above the age of 50. Using the data in the survey, is there substantial evidence ($\alpha = 0.05$
) that more than half of persons suﬀering from chronic pain are over 50 years of age?

Hypotheses:

$H_0: p \leq 0.5$

$H_a: p > 0.5$

Assumption Verification:

$n \cdot p_0 > 15$
```{r}
n <- 800
p_null <- 0.5
n * p_null > 15
```

$n \cdot (1 - p_0) > 15$
```{r}
n * (1 - p_null) > 15
```

Test Statistic:

```{r}
alpha <- 0.05
p_hat <- 424/n
se <- sqrt(p_null * (1 - p_null)/n)
z <- (p_hat - p_null)/se
z
```

Getting the rejection region:

```{r}
qnorm(1 - alpha)
```

Getting the p-value:

$p\_value = P(Z >$ `r z`$) = 1 - P(Z \leq$ `r z`$) =$ `r 1 - pnorm(z)`

Conclusion:

Is $z^* > z_{a}$?

`r z` > `r qnorm(1 - alpha)`

```{r}
z > qnorm(1 - alpha)
```

Is the p-value less than $\alpha$?

`r 1 - pnorm(z)` < `r alpha`

```{r}
1 - pnorm(z) < alpha 
```
So, at 5% signiﬁcance level, there is suﬃcient evidence to conclude that more than half of persons suﬀering from chronic pain are over 50 years of age.

## 1 Sample Mean

A study was conducted of 90 adult male patients following a new treatment for congestive heart failure. One of the variables measured on the patients was the increase in exercise capacity (in minutes) over a 4-week treatment period. The previous treatment regime had produced an average increase of $\mu$= 2 minutes. 

The researchers wanted to evaluate whether the new treatment had increased the value of $\mu$ in comparison to the previous treatment. The data yielded $\bar{x} = 2.17$ and $s = 1.05$. Using $\alpha=0.05$, what conclusions can you draw about the research hypothesis using rejection region approach?

Hypotheses:

$H_0: \mu \leq 2$

$H_a: \mu > 2$

Assumption Verification:

Since $n > 30$, we can assume this sample follows a normal distribution.

Test Statistic:

```{r}
alpha <- 0.05
n <- 90
x_bar <- 2.17
s <- 1.05
se <- s / sqrt(n)

t <- (x_bar - 2)/se
t
```

Getting the rejection region:

```{r}
qt(1 - alpha, df = n - 1) #1 - alpha since this is a right tailed test.
```

Getting the p-value:

$p\_value = P(T_{89, 0.05} >$ `r t`$) = 1 - P(T_{89, 0.05} \leq$ `r t`$) =$ `r 1 - pt(t, df = n - 1)`

Conclusion:

Is $t^* > T_{89, 0.05}$?

`r t` > `r qt(1 - alpha, df = n - 1)`

```{r}
t > qt(1 - alpha, df = n - 1)
```

Is the p-value less than $\alpha$?

`r 1 - pt(t, df = n - 1)` < `r alpha`

```{r}
1 - pt(t, df = n - 1) < alpha 
```
 
 At a significance level of 5%, we can conclude that the data does not support the hypothesis that the mean has been increased from 2.

# Hypothesis Testing with Two Samples

## Two Proportions

**Scenario:** Sludge is a dried product remaining from processed sewage and is often used as a fertilizer on agriculture crops. If the sludge contains a high concentration of certain heavy metals, such as nickel, the nickel may be at a concentration in the crops to be of danger to the consumer of the crop. A new method of processing sewage has been developed and an experiment is conducted to evaluate its effectiveness in removing heavy metals. Sewage of a known concentration of nickel is treated using both the new and old methods. 

One hundred tomato plants were randomly assigned to pots containing sewage sludge processed by one of the two methods. The tomatoes harvested from the plants were evaluated to determine if the nickel was at a toxic level. The results are as follows:

| Treatment&nbsp;&nbsp;       | Toxic&nbsp;&nbsp;  | Non-toxic&nbsp;&nbsp; | Total |
|:-----------------:|:---------:|:-------------:|:---------:|
| New             | 5     | 45        | 50    |
| Old             | 9     | 41        | 50    |
| *Total*         | 14    | 86        | 100   |

Hypotheses:

$H_0: p_1 = p_2$

$H_a: p_1 - p_2 < 0$

Assumption Verification:

```{r}
p_1 <- 5/50
p_2 <- 9/50
p_hat <- 14/100
assumptions <- c(50*p_1 >= 5, 50*(1- p_1) >= 5, 50*p_2 >= 5, 50*(1- p_2) >= 5)
assumptions
```


Test Statistic:

$z^* = \frac{p_1 - p_2}{\sqrt{\hat{p} (1 - \hat{p}(\frac{1}{n_1} + \frac{1}{n_2}))}}$

```{r}
alpha <- 0.05

SE <- sqrt(p_hat * (1 - p_hat) * (1/50 + 1/50))

z = (p_1 - p_2)/SE
z
```

Getting the rejection region:

```{r}
qnorm(alpha) # do not subtract 1 from alpha here since this is a left tailed test.
```

Getting the p-value:

$p\_value = P(Z \leq$ `r z`$) =$ `r pnorm(z)`

Conclusion:

Is $z^* < z_{a}$? 

(*note* the less than sign here due to this being a left tail test.)

`r z` < `r qnorm(alpha)`

```{r}
z < qnorm(alpha)
```

Is the p-value less than $\alpha$?

`r pnorm(z)` < `r alpha`

```{r}
pnorm(z) < alpha 
```
 
At a significance level of 5%, it seems that there is not significant evidence that the new treatment would have a lower proportion of plants having toxic levels of nickle.

## Two Means

According to the publication, High School Profile Report, in past years college-bound males have out-performed college-bound females on the mathematics portion of tests given by ACT Program. Samples of this year’s score yield the following data.

| Group   | C1 | C2 | C3 | C4 | C5 | C6 | C7 | C8 | C9 | C10 | C11 | C12 | C13 | C14 |
|:-------:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:---:|:---:|:---:|:---:|:---:|
| Males   | 34 | 34 | 30 | 18 | 24 | 16 | 15 | 26 | 13 | 21  | 24  | 11  | 11  | 15  |
| Females | 18 | 33 | 27 | 11 | 23 | 23 | 18 | 20 | 26 | 10  | 20  | 22  | 14  | 21  |


Does it appear that college-bound males are, on the average, still outperforming college-bound females on the mathematics portion of ACT tests? Set up the hypothesis. Indicate whether you are going to use 2-sample t-test or paired t-test for the problem. If you use 2-sample t-test, will you use pooled or non-pooled variances? What conditions will you check before you perform the test?

Hypotheses:

$H_0: \mu_{male} - \mu_{female} \leq 0$

$H_a: \mu_{male} - \mu_{female} > 0$

Assumptions:

Start by storing the study observations into a dataframe.

```{r}
males <- c(34,34,30,18,24,16,15,26,13,21,24,11,11,15)
females <- c(18,33,27,11,23,23,18,20,26,10,20,22,14,21)
df <- rbind(data.frame(values = males, group = "male"),
            data.frame(values = females, group = "female"))
```

The first assumption to check is whether we can assume equal variances. We
assess this with the rule of thumb: $0.5 < \frac{s_1}{s_2} <2.0$

In this instance: $\frac{s_1}{s_2} =$ `r sd(males)/sd(females)`

Since the ratio is within our threshold, we can proceed with pool variance.

The second assumption to check is whether the two sets follow a normal 
distribution.

```{r}
ggplot(df, aes(sample = values, color = group, fill = group)) +
  stat_qq_band(distribution = "norm", alpha = 0.1) +
  stat_qq_line(distribution = "norm", color = "red", alpha = .2) +
  stat_qq_point(distribution = "norm", color = "blue") +
  labs(title = "Normal Probability Plot (with Confidence Band)",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles") +
  facet_wrap(~group)+
  theme_minimal()
```

Test Statistic:

$t^* = \frac{\bar{x} - \bar{x}}{s_p \cdot \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}$

where,

$s_p = \sqrt frac{(n_1 -1)\cdot s_1^2 + (n_2 - 1 \cdot s^2_2)}{n_1 + n_2 -2}$

and 

$df = n_1 + n_2 -2$

```{r}
alpha <- 0.05
df <- length(males) + length(females) - 2
top = (length(males) - 1)*sd(males)**2 + (length(females) - 1)*sd(females)**2
s_p = sqrt(top/df)

t = (mean(males) - mean(females))/(s_p*sqrt(1/length(males)+1/length(females)))
t
```

Getting the rejection region:

```{r}
qt(1 - alpha, df)
```

Getting the p-value:

$p\_value = P(T_{26, 0.05} >$ `r t`$) =$ `r 1 - pt(t, df)`

Conclusion:

Is $t^* > T_{26, 0.05}$? 

`r t` > `r qt(1 - alpha, df)`

```{r}
t > qt(1 - alpha, df)
```

Is the p-value less than $\alpha$?

`r 1 - pt(t, df)` < `r alpha`

```{r}
1 - pt(t, df) < alpha 
```
 
At a significance level of 5%, there is not enough evidence to conclude that, on average, college-bound males score higher on the ACT Math compared to college-bound females.

## Paired Means

**Scenario:** Eleven tires were each measured for tread wear by two methods, one based on weight and the other on groove wear. Here is the data in thousands of miles:

| Group  | C1   | C2   | C3   | C4   | C5   | C6   | C7   | C8   | C9   | C10  | C11  |
|:------:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|
| Weight | 30.5 | 30.9 | 31.9 | 30.4 | 27.3 | 20.4 | 24.5 | 20.9 | 18.9 | 13.7 | 11.4 |
| Groove | 28.7 | 25.9 | 23.3 | 23.1 | 23.7 | 20.9 | 16.1 | 19.9 | 15.2 | 11.5 | 11.2 |

Does it appear the two methods, on the average, give different results? Set up the hypotheses. Indicate whether you are going to use 2-sample t-test or paired t-test for the problem. If you use 2-sample t-test, will you use pooled or non-pooled variances? What conditions will you check before you perform the test?

Hypotheses:

$H_0: \mu_d = 0$

$H_a: \mu_d \neq 0$

Assumptions:

Verify that $d$ follows a normal distribution

```{r}
Weight <- c(30.5,30.9,31.9,30.4,27.3,20.4,24.5,20.9,18.9,13.7,11.4)
Groove <- c(28.7,25.9,23.3,23.1,23.7,20.9,16.1,19.9,15.2,11.5,11.2)
d = Weight - Groove

ggplot(data.frame(values = d), aes(sample = values)) +
  stat_qq_band(distribution = "norm", alpha = 0.2, fill = "lightblue") +
  stat_qq_line(distribution = "norm", color = "red", alpha = .2) +
  stat_qq_point(distribution = "norm", color = "blue") +
  labs(title = "Normal Probability Plot (with Confidence Band)",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles") +
  theme_minimal()
```

Test Statistic:

The test statistic is given by

$t^* =\frac{\bar{d}}{\frac{s_d}{\sqrt{n}}}$

with the degrees of freedom being given by $df = n(pairs)  - 1$

```{r}
alpha = 0.05/2 # since this is a two tailed test.
t = mean(d)/(sd(d)/sqrt(length(d)))

df = length(d) - 1

t
```

Getting the rejection region:

```{r}
qt(1 - alpha, df)
```

Getting the p-value:

$p\_value = P(T_{10, 0.025} >|$ `r t`$|) =$ `r 2*(1 - pt(t, df))`

Conclusion:

Is $t^* > T_{10, 0.05}$? 


`r t` > `r qt(1 - alpha, df)`

```{r}
t > qt(1 - alpha, df)
```

Is the p-value less than $\alpha*2$?

`r 2*(1 - pt(t, df))` < `r alpha*2`

```{r}
2*(1 - pt(t, df)) < alpha *2
```
 
There is statistical evidence to conclude that there is a difference in average tread wear between the weight and groove methods.

# Chi-Square Test for Independence

**Scenario:** The fire department in a large city is examining its promotion policy to assess if there is the potential for an age discrimination lawsuit. A random sample of 248 promotion decisions over the past years yields the following information:

| Promotion Decision | Under 30 | 30–39 | 40–49 | 50 or Older |
|--------------------|----------|-------|-------|-------------|
| Promoted           | 9        | 29    | 34    | 12          |
| Not Promoted       | 41       | 39    | 46    | 38          |

```{r}
df <- data.frame(n = c(9,41), decision = c("Promoted", "Not Promoted"), age = "Under 30")
df <- rbind(df, data.frame(n = c(29,39), decision = c("Promoted", "Not Promoted"), age = "30-39"))
df <- rbind(df, data.frame(n = c(34,46), decision = c("Promoted", "Not Promoted"), age = "40-49"))
df <- rbind(df, data.frame(n = c(12,38), decision = c("Promoted", "Not Promoted"), age = "50 or Older"))
```

(a) Provide a graph of the promotion information.

```{r}
ggplot(df, aes(x = age, y = n, color = age, fill = age)) + geom_col(alpha = 0.5) + facet_wrap(~decision)  + coord_flip()
```

(b) Is the promotion decision for the fireman related to the age of fireman?

Hypotheses:

$H_0:$ Age and promotion is independent

$H_a:$ Age and promotion is dependent


Getting the test statistic:

$\chi^{2*} = \sum{\frac{(O_i - E_i)^2}{E_i}}$

```{r}
alpha <- 0.05/2
degrees <- (nrow(df) - 1) * (ncol(df) - 1 )
degrees <- 3
df$expected_value <- 0
df$chi_sq <- 0

for (i in 1:nrow(df)){
  decision_type <- df[i,]$decision
  age_type <- df[i,]$age
  # sum along each row and column :)
  expected <- sum(df[df$age == age_type,]$n) * sum(df[df$decision == decision_type,]$n)
  expected <- expected/sum(df$n)
  df[i,]$expected_value <- expected
  df[i,]$chi_sq <- ((df[i,]$n - expected)**2)/expected
}
chi_sq <- sum(df$chi_sq)
```

$\chi^{2*} =$ `r chi_sq`

```{r}
reactable(df,searchable = TRUE,sortable = TRUE,defaultPageSize = 5, bordered = TRUE, highlight = TRUE)
df <- 10
```

Getting the rejection region:

The rejection region is: $\chi^{2}_{a/2} =$ `r qchisq(alpha, degrees)` 

with degrees of freedom: `r degrees`

Getting the p-value:

$p\_value = P(\chi^{2}_{3, 0.025}> |$ `r chi_sq`$|) =$ `r 1 - pchisq(chi_sq, degrees)`

Conclusion:

Is $\chi^{2*} > \chi^{2}_{3, 0.025}$? 

`r chi_sq` > `r qchisq(alpha, degrees)`

```{r}
chi_sq > qchisq(alpha, degrees)
```

Is the p-value less than $\alpha$?

`r 1 - pchisq(chi_sq, degrees)` < `r alpha`

```{r}
1 - pchisq(chi_sq, degrees) < alpha 
```
 
There is statistical evidence to conclude that there is a difference in average tread wear between the weight and groove methods.

(c) What is the population to which your conclusion in part (b) is applicable?

All the employees at the fire department in that city, at the time of sampling.

(d) What are some other variables, beside age, that needed to be addressed in an age discrimination analysis?

Answers may vary. Examples include performance metrics, number of fires in the area (to determine need), city size, credentials, position (whether there is potential for promotion), etc..

# Linear Regression

## Confidence Interval Example

**Scenario:** The following data are 11 randomly selected people’s maximum heart rate (Y) and their age (X)

| Age | 30  | 38  | 41  | 38  | 29  | 39  | 46  | 41  | 42  | 24  | 49  |
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| Rate| 186 | 183 | 171 | 177 | 191 | 177 | 174 | 176 | 171 | 196 | 168 |



a)  Plot the data. Does a linear equation relating Y to X appear plausible?

```{r}
Age <- c(30 ,38 ,41 ,38 ,29  ,39,46 ,41 ,42  ,24,49)
Rate <- c(186,183,171,177,191,177,174,176,171,196,168)
df <- data.frame(age = Age, rate = Rate)
ggplot(df, aes(x = age, y = rate)) + geom_point()
```


b)  What is the regression equation?
```{r}
model <- lm(Rate ~ Age, data = df)

fits <- fitted(model)
resids <- resid(model)
df$Fits <- fits
df$Residuals <- resids

summary(model)
```

The equation is: $y = -1.1230x + 221.6634$

c)  Interpret the coefficient of X in the context of this problem.

The coefficient of X is the coefficient of age, which is –1.123. The coefficient is the slope of the regression equation which means: for every one year increase in age, the estimated increase in maximum heart rate is: -1.123 (equivalently, the estimated decrease in maximum heart rate is 1.123.)

d)  Locate the residual standard deviation.

It is on the line:

`## Residual standard error: 3.009 on 9 degrees of freedom`

The residual standard deviation is 3.

e)  Will you use the regression equation to predict the maximum heart rate of a person who is 4 years old? If so, what is the value?

No, because age = 4 is out of scope of the X-values in the data that are used to obtain the regression equation. It would be inappropriate to extrapolate the maximum heart rate using the regression equation for a person who is 4 years old.

f)  Calculate a 95% confidence interval for the true slope.

The equation for the confidence interval is:

$\hat{\beta}_1 \pm t_{a/2}\cdot \hat{SE}(\hat{\beta}_1) = -1.1230 \pm$ `r qt(1-0.025, 9)` $\cdot 0.1271 \rightarrow$ (`r -1.1230 - qt(1-0.025, 9)*0.1271`, `r -1.1230 + qt(1-0.025, 9)*0.1271`) 


g)  Find a 95% interval for the mean rate when age is 35 and a 95% interval for the rate when age is 35.

```{r}
# Fit the model
model <- lm(Rate ~ Age, data = df)

# New data for age = 35
new_data <- data.frame(Age = 35)

# 95% confidence interval for the mean Rate
conf_int <- predict(model, newdata = new_data, 
                    interval = "confidence", level = 0.95)

# 95% prediction interval for an individual Rate
pred_int <- predict(model, newdata = new_data, 
                    interval = "prediction", level = 0.95)
```

The *confidence interval* is: (`r conf_int[1, "lwr"]`, `r conf_int[1, "upr"]`)

The *prediction interval* is: (`r pred_int[1, "lwr"]`, `r pred_int[1, "upr"]`) 

h)  Compute R2 and give an interpretation for it.

The $R^2$ term comes from the line:`## Multiple R-squared:  0.8967`

It means that the linear regression model using age as the predictor can explain 89.67% of the variability in maximum heart rate. 

## Hypothesis Testing

**Scenario:** Athletes are constantly seeking measures of the degree of their cardiovascular fitness prior to a major race. Athletes want to know when their training is at a level that will produce a peak performance. One such measure of fitness is the time to exhaustion from running on a treadmill at a specified angle and speed. The important question is then, Does this measure of cardiovascular fitness translate into performance in a 10-km running race? Twenty experienced distance runners who professed to be in top condition were evaluated on the treadmill and then had their times recorded in a 10-km race. The data, in minutes, were as follows:

| Treadmill | 7.5 | 7.8 | 7.9 | 8.1 | 8.3 | 8.7 | 8.9 | 9.2 | 9.4 | 9.8 | 10.1 | 10.3 | 10.5 | 10.7 | 10.8 | 10.9 | 11.2 | 11.5 | 11.7 | 11.8 |
|:---------:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|
| TenK      | 43.5| 45.2| 44.9| 41.1| 43.8| 44.4| 38.7| 43.1| 41.8| 43.7| 39.5 | 38.2 | 43.9 | 37.1 | 37.7 | 39.2 | 35.7 | 37.2 | 34.8 | 38.5 |


a) Create a scatterplot of the data. Does there appear to be a linear relation-ship?

```{r}
Treadmill <- c(7.5,7.8,7.9,8.1,8.3,8.7,8.9,9.2,9.4,9.8,10.1,10.3,10.5,10.7,10.8,10.9,11.2,11.5,11.7,11.8)
TenK <- c(43.5,45.2,44.9,41.1,43.8,44.4,38.7,43.1,41.8,43.7,39.5,38.2,43.9,37.1,37.7,39.2,35.7,37.2,34.8,38.5)
df <- data.frame(Treadmill = Treadmill, TenK = TenK)
ggplot(df, aes(x = Treadmill, y = TenK)) + geom_point()
```

b) Use software to find the best fit regression equation and write that equation below.

```{r}
model <- lm(TenK ~ Treadmill, data = df)

fits <- fitted(model)
resids <- resid(model)
df$Fits <- fits
df$Residuals <- resids

summary(model)
```

The equation is: $y = 58.8158 + -1.8673x$

c) What is the interpretation of the slope of this line as it relates to this problem?

For an increase in treadmill time of one minute, the estimated average 10-km race time decreases by 1.867 minutes.

d) Using the regression equation in Part b, what is the predicted 10-km race time of a runner with a treadmill time of 10 minutes? Provide an interpretation of this prediction.

With a treadmill time of 10 minutes, the prediction is: `r 58.8158 + -1.8673 * 10`

e) Using the regression output, find a 95% confidence interval for the slope of the regression line and provide an interpretation of this interval.

$\hat{\beta}_1 \pm t_{a/2}\cdot \hat{SE}(\hat{\beta}_1) = -1.8673 \pm$ `r qt(1-0.025, 18)` $\cdot 0.3462 \rightarrow$ (`r -1.8673 - qt(1-0.025, 18)*0.3462`, `r -1.8673 + qt(1-0.025, 18)*0.3462`) 

f) Conduct a test of significance for the slope at a 5% level of significance.Be sure to include your hypotheses, p-value, decision, and conclusion.

Hypothesis:

$H_0: \beta_1 \geq 0$

$H_a: \beta_1 < 0$

Assumptions:

**Linearity**: The first assumption to check is whether there appears to be a linear relationship between X and Y.
```{r}
ggplot(df, aes(x = Treadmill, y = TenK)) + geom_point() + geom_smooth(method = "lm", se = FALSE, color = "red")
```

**Independence of Errors**: The second assumption requires that there is not a relationship between residuals and weight.

```{r}
ggplot(df, aes(x = Fits, y = Residuals)) + geom_point() + 
         geom_smooth(method = "lm", se = FALSE, color = "red") +
         geom_hline(yintercept = 0, color = "grey", linetype = "dashed")
```

**Normality of Error**: The residuals must approximately normally distributed.

```{r}
ggplot(df, aes(sample = Residuals)) +
  stat_qq_band(distribution = "norm", alpha = 0.2, fill = "lightblue") +
  stat_qq_line(distribution = "norm", color = "red", alpha = .2) +
  stat_qq_point(distribution = "norm", color = "blue") +
  labs(title = "Normal Probability Plot of Residuals (with Confidence Band)",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles") +
  theme_minimal()
```

**Equal Variances**: The variance of the residuals is the same for all values of $X$.
```{r}
ggplot(df, aes(x = Fits, y = Residuals)) + geom_point() + 
         geom_smooth(method = "lm", se = FALSE, color = "red") +
         geom_hline(yintercept = 0, color = "grey", linetype = "dashed")
```

Test Statistic:

The test statistic is given by:

$t^* = \frac{\hat{\beta}_1}{\hat{SE}(\hat{\beta}_1)} = \frac{-1.8673}{0.3462} =$ `r -1.8673/0.3462`

Rejection Region:
```{r}
t <-  -1.8673/0.3462
degrees <- nrow(df) - 2
degrees <- 18

alpha = 0.05
qt(alpha, degrees)
```

Getting the p-value:

$p\_value = P(T_{18, 0.05} >|$ `r t`$|) =$ `r pt(t, degrees)`

Conclusion:

Is $t^* < T_{18, 0.05}$? 


`r t` < `r qt(alpha, degrees)`

```{r}
t < qt(alpha, degrees)
```

Is the p-value less than $\alpha$?

`r pt(t, degrees)` < `r alpha`

```{r}
pt(t, degrees) < alpha 
```
 
We reject the null hypothesis and conclude that treadmill time is a significant linear predictor of 10-km race time for top conditioned runners.

g) From the output, what is the estimate standard deviation of the model(i.e.σ)?

We get the estimated standard deviation of the model from the line: `## Residual standard error: 2.102 on 18 degrees of freedom`

which is the square root of the estimated model variance, MSE. For this data, the estimated model standard deviation is 2.102

h) What is the coefficient of determination and how is it interpreted for this study?

This is given in the output as ’R-sq’ which is 61.77%. This means that about 61.77% of the variation in 10-km race time for top conditioned runners is explained by treadmill time (or explained by the regression model).

i) What is the correlation between 10-km and treadmill times?

 The correlation is found by taking the square root of the R-sq as a pro-portion with the sign of the correlation taken by the direction of the slope in the regression line. Here, the correlation would be the square root of 0.6177 and be negative. The correlation is then,  -0.786.

# Analysis of Variance

To compare the heights of Soprano, Alto, Tenor and Bass singers, data have been collected from the NY Choral Society in 1979.

Determine whether these four groups have the same mean height. Use $\alpha = 0.05$

Hypotheses:

$H_0: \forall_{i, j \in \{ A,A,T,B\}, i \neq j} \mu_i = \mu_j$


$H_a: \exists_{i, j \in \{ A,A,T,B\}, i \neq j} \mu_i \neq \mu_j$


Data Loading:

```{r}
df <- read.xlsx("../Sample_Data/singers-1 (1).xlsx", sheet = 1)
reactable(df,searchable = TRUE,sortable = TRUE,defaultPageSize = 5, bordered = TRUE, highlight = TRUE)
```

Data Modeling:

We have to pivot the data to a longer format.

```{r}
df_long <- df %>% pivot_longer(cols = everything(), names_to = "Group", values_to = "Value")
reactable(df_long,searchable = TRUE,sortable = TRUE,defaultPageSize = 5, bordered = TRUE, highlight = TRUE)
model <- aov(Value ~ Group, data = df_long)
anova_summary <- summary(model)
f_value <- anova_summary[[1]][["F value"]][1]
anova_summary
```

Getting the additional output that minitab creates.
```{r}
#aggregate(Value ~ Group, data = df_long, mean)
#df_long$Fits<- model$fitted.values   # fitted means
#df_long$Residuals<- model$residuals       # residuals
TukeyHSD(model)
```

The test statistic is the F-value of `r f_value`

Rejection Region:
```{r}
alpha = 0.05/2
# from the summary
df1 <- 3 # group degrees of freedom
df2 <- 126 # error degrees of freedom

qf(alpha, df1 = df1, df2 = df2)
```

Getting the p-value:

$p\_value = P(F_{3,126, 0.025} >|$ `r f_value`$|) =$ `r 2*(1 - pf(f_value, df1 =df1, df2 = df2))`

Conclusion:

Is $f^* > F_{3,126, 0.025} $? 


`r f_value` > `r qf(1 - alpha, df1 = df1, df2 = df2)`

```{r}
f_value > qf(1 - alpha, df1 = df1, df2 = df2)
```

Is the p-value less than $\alpha$?

`r 2*(1 - pf(f_value, df1 =df1, df2 = df2))` < `r alpha`

```{r}
2*(1 - pf(f_value, df1 =df1, df2 = df2)) < alpha 
```

We would conclude that the sample data provides statistical evidence that there is a difference in the mean heights across the four types of singers