---
title: "STAT501_L78_Drill_Analysis"
author: "Ada Lazuli"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: spacelab
    highlight: tango
    toc: true
    toc_depth: 3
    toc_float: true
    code_folding: hide
    code-tools: true
  pdf_document:
    toc: true
    toc_depth: '3'
  word_document:
    toc: true
    toc_depth: '3'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("/home/maple/CodeProjects/Statistics/STAT501")
```

```{r, warning=FALSE, message=FALSE}
library(dplyr)
library(ggplot2)
library(qqplotr)
library(tidyr)
library(reactable)
library(openxlsx)
library(nortest) # For anderson-darling test
library(lmtest) # needed for Breusch-Pagan test
```

# Drill Data Analysis

## Data and Background

A laboratory that tests consumer products selected a variety of drills to compare how likely each will overheat under certain conditions.

The response variable for analysis is **AVTEM** = mean temperature of drill. The dataset includes predictor variables measuring factors to control the drillâ€™s resistance to overheating. 

In particular, **IT** is thickness of insulation, **QW** is an assessment of wire quality in the drill motor, **VS** is the size of the vent used in the drill motor, **I2** is **IT** centered and squared, and **Q2** is **QW** centered and squared.

```{r}
df <- read.csv("~/Downloads/HW6Drill.csv")
head(df)
```

## Modeling

```{r}
model <- lm(AVTEM ~ IT + I2 + VS + QW + Q2, data = df)
summary(model)
```


```{r}
anova(model)
```


Based on the ANOVA output, the sum of squared errors is 1324.2 with degrees of freedom 84

## Analysis of QW and Q2

We need to perform a general linear F-test to determine whether QW and Q2 provide significant information about the response.

$H_0: QW=Q2= 0 \\ H_a: (QW \neq 0) \vee (Q2 \neq 0)$

$$
F^* = \frac{\frac{SSE_R - SSE_F}{P_F - P_R}}{\frac{SSE_F}{n - P_F}}
$$
$$
SSE_R = 1324.2 + 0.3 + 18.2 = 1342.7, \quad P_F = 6,\quad P_R = 4, \quad SSE_F = 1324.2
$$
So,

$$
F^* = \frac{(1342.7-1324.2)/(6-4)}{1324.2/84} = 0.5868
$$

Then, to get the **p-value**,

```{r}
1 - pf(0.5868, 2, 84)
```

Since $P(F_{2,84} \leq x ) = 0.5583671$, we fail to reject the null hypothesis.

At a significance level of 5%, there is not evidence that QW and Q2 contribute significantly after considering the rest of the predictors.

## LINE Assumption Verification On Reducted Model

```{r}
model <- lm(AVTEM ~ IT + I2 + VS, data = df)
df$resid <- resid(model)
df$fitted <- fitted(model)
summary(model)
```

```{r}
anova(model)
```

### Normalcy

Starting with the assumption or normalcy.

```{r}
ggplot(df, aes(x=resid)) + geom_histogram(alpha=0.3)
```
```{r}
ggplot(df, aes(sample = resid)) +
  stat_qq_band(distribution = "norm", alpha = 0.2, fill = "lightblue") +
  stat_qq_point(distribution = "norm", color = "blue") +
  stat_qq_line(distribution = "norm", color = "grey", size = 1) + 
  labs(title = "Normal Probability Plot",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles")
```

Based on the probability plot and histogram, it seems like the residuals are approximately normally distributed.

The Anderson-Darling test will allow for confirmation.

$H_0: \text{The errors follow a normal distribution}\\ H_a: \text{The errors do not follow a normal distribution}$

```{r}
ad.test(df$resid)
```

With a $\text{p-value} = 0.2866$, there is not enough evidence to reject the null hypothesis.

### Linearity and Equal Variance

```{r}
ggplot(df, aes(x = IT, y=resid)) + geom_point() + ylab("Residuals") + xlab("Fits") + ggtitle("Residual Vs Fits plot for IT") +  geom_hline(yintercept = 0, color = "grey")
```

```{r}
ggplot(df, aes(x = VS, y=resid)) + geom_point() + ylab("Residuals") + xlab("Fits") + ggtitle("Residual Vs Fits plot for VS") +  geom_hline(yintercept = 0, color = "grey")
```

Based on the scatter plots, it seems that the assumptions for linearity and equal variance are met.


Going further with a a **Breusch-Pagan** test for equal variance:

#### Manual BP Test

```{r}
df$e <- resid(model)
df$e2 <- df$e^2
```

```{r}
aux <- lm(e2 ~ IT + I2 + VS, data = df)
anova(aux)
```

$H_0: \forall_{i \in \{1,2,3\}} \gamma_i = 0\\H_a: \exists_{i \in \{1,2,3\}} \gamma_i \neq 0$

Our test statistic is given by

$$
\chi^2_{bp} = \frac{SSR^*}{2} \div \Bigg(\frac{SSE_F}{n}\Bigg)^2 
$$
Where our degrees of freedom are based on the number of predictors. In this case $df = 3$. **Note:** There is some conflicting information on whether to divide the $SSR^*$ term by 2 when conducting MLR.

$$
SSR^* = 9 + 356 + 171 = 536, \quad SSE_F = 1342.7, \quad n=90, \quad df  = 3
$$

So,

$$
\chi^2_{bp} = \frac{SSR^*}{2} \div \Bigg(\frac{SSE_F}{n}\Bigg)^2 = \frac{536}{2} \div \Bigg(\frac{1342.7}{90}\Bigg)^2 =  1.20
$$

The critical region for this test is:

```{r}
qchisq(0.95,3)
```

And our p-value is:

```{r}
1 - pchisq(1.204098,3)
```

So we fail to reject the null hypothesis.

At a significance level of 5%, there no evidence that the constancy of residuals differ from 0 for each of the predictors.

#### Using lmtest package

```{r}
bptest(model, ~ IT + I2 + VS, data = df)
```

### Confidence and Prediction Intervals

We need to perform a confidence interval for drills with the characteristics of:
1. $IT = 6$
2. $I2 = 4$
3. $VS = 10$


```{r}
new <- data.frame(IT = 6, I2 = 4, VS = 10)

predict(model, newdata = new, interval = "confidence", level = 0.95)
```

```{r}
new <- data.frame(IT = 6, I2 = 4, VS = 10)

predict(model, newdata = new, interval = "prediction", level = 0.95)
```